---
title: "Time Series with R"
author: "Sara Geremia"
date: "2024-05-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, 
                      error = FALSE, message = FALSE)

```

Let's work with dates and times in R.

# Time Series

A time series is a sequence of data in chronological order. It is very common for any type of data to be recorded sequentially, or over time. And we find time series data everywhere, and especially in Financial and Economic applications. Examples include: monthly values of the House Price Index.


We will focus on the `lubridate` package, which makes it easy to work with dates and times in R.

```{r}
library(tidyverse)
library(lubridate)
```

## Create dates/times

You should always use the simplest possible data type that works for your needs:
avoid date-time data type if not necessary.
Date-times are substantially more complicated because of the need to handle time zones.

To get the current date or date-time you can use `today()` or `now()`:

```{r}
today()

now()
```

Date/time data often comes as strings. 
Identify the order in which year, month, and day appear in your dates, then use the 
appropriate functions:

```{r}
ymd("2024-05-24")
mdy("May 24th, 2024")
dmy("24-May-2024")
```

These functions also take unquoted numbers. 

```{r}
ymd(20240524)
```

Sometimes youâ€™ll have the individual components of the date-time spread across multiple columns:

```{r}
# load data
# install.packages("nycflights13")
library(nycflights13)
```


```{r}
Sys.setlocale("LC_ALL", "en_US")

flights <- flights %>% 
  select(year, month, day, hour, minute) %>% 
  mutate(dep_time = make_datetime(year, month, day, hour, minute))
  
```

## Visualize dates/times

The most common first step when conducting time series analysis is to display your time series dataset in a visually intuitive format. 

The defining feature is that time is indexed on the horizontal axis, and the observations are shown from the first, on the left, to the last, on the right. A line is commonly added to connect neighboring observations, to improve interpretability and to emphasize any trends or patterns.

```{r}
flights %>% 
  ggplot(aes(dep_time)) + 
  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day
```

```{r}
flights %>% 
  filter(dep_time < ymd("2013-01-02")) %>% 
  ggplot(aes(dep_time)) + 
  geom_freqpoly(binwidth = 600) # 600 s = 10 minutes
```

We can use wday() to see that more flights depart during the week than on the weekend:

```{r}
flights %>% 
  mutate(wday = wday(dep_time, label = TRUE)) %>% 
  ggplot(aes(x = wday)) +
    geom_bar()
```

Some time series data is exactly evenly spaced.
Some time series data is only approximately evenly spaced. 

The analysis of time series data proceeds with some simplifying assumptions: The first assumption is that consecutive observations are equally spaced. Secondly, a discrete-time observation index is applied. In practice, this may only hold approximately, and sometimes data may be missing. For example, daily log returns on a stock may only be available for weekdays, and data may not be available for certain holidays. Monthly House Price Index values are equally spaced by month, but not by days.

Sometimes there are missing values in time series data, denoted NA in R, and it is useful to know their locations. It is also important to know how missing values are handled by various R functions. Sometimes we may want to ignore any missingness, but other times we may wish to impute or estimate the missing values.

The mean() function calculates the sample mean, but it fails in the presence of any NA values. Use mean(___, na.rm = TRUE) to calculate the mean with all missing values removed. It is common to replace missing values with the mean of the observed values. 

```{r}

flights_dt <- flights %>% 
  count(year, month, day) %>% 
  mutate(dep_time = make_date(year, month, day)) %>% 
  filter(dep_time < ymd("2014-01-01"))


flights_dt_na <- flights_dt
flights_dt_na$n[flights_dt_na$dep_time > ymd("2013-01-31") & flights_dt_na$dep_time < ymd("2013-02-28")] <- mean(flights_dt_na$n, na.rm = TRUE)


```

```{r}
ggplot() +
  geom_line(data = flights_dt, aes(x = dep_time, y = n), color = "red", linetype = "dotted")+
  geom_line(data = flights_dt_na, aes(x = dep_time, y = n)) 
```

 Does this simple data imputation scheme appear adequate?
 
 Based on your plot, it seems that simple data imputation using the mean is not a great method to approximate what's really going on in the flights data.
 

## Create Time Series

 
Given a vector of numbers you can apply the ts() function to create a time series object. Such objects are of the `ts` class. They represent data that is at least approximately evenly spaced over time.
 
```{r}
data_vector <- c(10, 6, 11, 8, 10, 3, 6, 9)
print(data_vector)
plot(data_vector)

```

```{r}
time_series <- ts(data_vector)
```


When you plot the result using the plot() function the time index and label is automatically added to the horizontal axis. By default, R uses a simple observation index starting from 1 as the time index.

```{r}
print(time_series)
plot(time_series)
```

If you want the time series to start in the year 2001 with 1 observation per year you should apply the ts() function with the additional arguments start = 2001 and frequency = 1 as shown. Now when you plot the result you can see an updated time axis, running from 2001 through 2008.

```{r}
time_series <- ts(data_vector, start = 2001, frequency = 1)
plot(time_series)
```

A time series object is a vector (univariate) or matrix (multivariate) with additional attributes, including time indices for each observation, the sampling frequency and time increment between observations, and the cycle length for periodic data. 

The advantage of creating and working with time series objects of the ts class is that many methods are available for utilizing time series attributes, such as time index information.

Let's consider the eu_stocks dataset (available in R by default as EuStockMarkets). This dataset contains daily closing prices of major European stock indices from 1991-1998, specifically, from Germany (DAX), Switzerland (SMI), France (CAC), and the UK (FTSE). The data were observed when the markets were open, so there are no observations on weekends and holidays. We will proceed with the approximation that this dataset has evenly spaced observations and is a four dimensional time series.

 
```{r}
library(datasets)
data(EuStockMarkets)

```

```{r}
# Check whether EuStockMarkets is a ts object
is.ts(EuStockMarkets)

# View the start, end, and frequency of EuStockMarkets
start(EuStockMarkets)
end(EuStockMarkets)
frequency(EuStockMarkets)



# Generate a simple plot of EuStockMarkets
plot(EuStockMarkets)

# Use ts.plot with EuStockMarkets
ts.plot(EuStockMarkets, col = 1:4, xlab = "Year", ylab = "Index Value", main = "Major European Stock Indices, 1991-1998")

# Add a legend to your ts.plot
legend("topleft", colnames(EuStockMarkets), lty = 1, col = 1:4, bty = "n")

```

Let's try to make a better visualization.

```{r}
library(forecast)
autoplot(EuStockMarkets) + 
  theme_minimal() +
  labs(x = "Year", y = "Index Value", title = "Major European Stock Indices, 1991-1998") +
  scale_x_discrete(limits = seq(1991, 1998))
```


# Removing Trends

## Logarithm Transformations

The logarithmic function `log()` is a data transformation that can be applied to positively valued time series data. It slightly shrinks observations that are greater than one towards zero, while greatly shrinking very large observations. This property can stabilize variability when a series exhibits increasing variability over time. It may also be used to linearize a rapid growth pattern over time.

```{r}
rapid_growth <- ts(c(
  505.9547, 447.3556, 542.5831, 516.0634, 506.9599, 535.0162, 496.9291,
  497.5626, 577.2483, 536.8560, 541.2459, 473.4978, 550.9890, 569.4106,
  522.9152, 487.2002, 594.6108, 591.1740, 615.9868, 621.3175, 607.1250,
  587.0367, 554.1554, 644.1172, 509.7000, 607.0943, 603.5512, 613.6216,
  544.9143, 670.8118, 687.1316, 615.5817, 711.1873, 694.2979, 681.9293,
  659.1403, 642.7021, 601.5301, 666.7623, 650.9657, 606.0913, 696.6788,
  641.6025, 855.7719, 667.3291, 573.4914, 791.7333, 751.5914, 610.7948,
  624.6503, 833.2990, 639.8867, 736.8283, 772.2923, 686.8865, 667.7631,
  712.9415, 918.1838, 656.1089, 700.4972, 683.4933, 781.7380, 715.6843,
  808.2875, 820.7795, 656.8856, 733.3400, 773.5387, 641.2027, 932.2119,
  680.6766, 988.2828, 664.8986, 813.5283, 883.4088, 924.2749, 969.4321,
  777.3293, 880.9984, 971.3583, 902.9584, 1020.7457, 1075.1483, 886.1707,
  889.6322, 950.3908, 878.0395, 1043.7676, 901.1090, 1079.6584, 933.9054,
  921.9433, 870.8071, 811.1398, 1004.2677, 1008.1758, 1189.4893, 751.9706,
  947.4753, 886.5153, 1074.8943, 1101.1307, 1130.1855, 975.8495, 948.1610,
  1177.8227, 1227.1271, 976.9957, 836.7089, 1323.6047, 852.3532, 1200.8262,
  1274.4788, 1349.2614, 1102.6334, 1324.8566, 1268.7187, 1058.2289, 1204.0872,
  1084.6503, 1284.4305, 1195.2843, 1058.4262, 1188.0577, 1166.5934, 1064.6946,
  1429.0685, 1070.8528, 1539.3305, 1467.1571, 1127.7058, 1296.0717, 1555.2741,
  1332.9037, 1315.4236, 1189.2462, 1482.4339, 1240.9287, 1237.7720, 1468.6083,
  1328.5457, 1589.5078, 1373.1630, 1503.5563, 1659.9376, 1704.6137, 1550.4638,
  1625.8026, 1873.8582, 1370.6209, 1439.7114, 1447.4369, 1579.9158, 1681.2571,
  1661.6059, 1311.8468, 1326.0308, 1323.0995, 1550.4863, 1606.2042, 1768.5401,
  1509.8368, 1592.1086, 1627.6188, 1544.6329, 1439.5234, 1682.3518, 1850.7097,
  1673.3801, 1832.4272, 1672.2672, 1781.5768, 1659.2899, 1970.0389, 2044.7124,
  1929.0902, 1891.7042, 1487.1577, 2013.8722, 1796.7886, 1977.0183, 1516.9552,
  1650.6039, 1523.2834, 1696.6181, 1627.2609, 1787.2968, 1567.2874, 1881.9963,
  2318.9833, 1941.9879, 1820.2797, 2154.8123, 2261.5471, 2052.2214, 2079.1710,
  2010.0609, 2145.2606, 1775.3008, 2013.4070
))
```

```{r}
autoplot(rapid_growth) + 
  theme_minimal() +
  labs(x = "Year", y = "Index Value") 
```

Apply the log() function to rapid_growth

```{r}
linear_growth <- log(rapid_growth)

autoplot(linear_growth) + 
  theme_minimal() +
  labs(x = "Year", y = "Index Value") 
```

The logarithmic transformation helps stabilize your data by inducing linear growth over time.

## First Difference Transformations

The first difference transformation of a time series $Y_t$
 consists of the differences (changes) between successive observations over time, that is 
$Y_t - Y_{t-1}$.

Differencing a time series can remove a time trend. The function `diff()` will calculate the first difference or change series. A difference series lets you examine the increments or changes in a given time series. It always has one fewer observations than the original series.

```{r}
dz <- diff(linear_growth)

autoplot(dz) + 
  theme_minimal() +
  labs(x = "Time", y = "Index Value") 
```

By removing the long-term time trend, you can view the amount of change from one observation to the next.

## $s$ Difference Transformations

For time series exhibiting seasonal trends, seasonal differencing can be applied to remove these periodic patterns. For example, monthly data may exhibit a strong twelve month pattern. In such situations, changes in behavior from year to year may be of more interest than changes from month to month, which may largely follow the overall seasonal pattern.

```{r}
y <- ts(c(
  -4.198033, 9.569009, 5.175143, -9.691646, -3.215294, 10.843669,
  6.452159, -10.833559, -2.235351, 10.119833, 6.579646, -8.656565,
  -2.515001, 9.837434, 7.386194, -8.243504, -4.264033, 8.898861,
  8.544336, -8.066913, -4.023025, 9.822679, 7.772852, -6.587777,
  -3.459171, 10.613851, 7.374450, -5.798715, -1.204711, 11.429236,
  7.570047, -4.968384, -2.003787, 11.941348, 9.406672, -4.396585,
  -1.555579, 12.599877, 8.502916, -3.728968, -2.827000, 13.375981,
  8.128941, -3.149249, -2.799473, 13.710570, 6.755217, -3.779744,
  -3.768274, 13.625336, 6.537931, -3.249098, -5.024191, 13.355373,
  6.931161, -3.527354, -5.197329, 11.579791, 7.162449, -1.894607,
  -5.777797, 12.482695, 6.208088, -3.434038, -7.080721, 11.413656,
  6.741990, -3.532376, -8.393542, 12.507261, 6.473175, -3.745246,
  -9.426209, 12.380817, 8.048243, -2.831528, -7.301893, 12.765838,
  8.223699, -4.448131, -6.963558, 12.034005, 7.574925, -5.402218,
  -6.568198, 10.896482, 7.276571, -4.037873, -6.723013, 12.180815,
  8.285162, -4.159342, -6.360670, 12.753018, 8.665912, -5.440538,
  -4.874932, 12.600197, 8.162589, -6.539572
))

autoplot(y) + 
  theme_minimal() +
  labs(x = "Time", y = "Index Value") 
```
The function `diff(..., lag = s)` will calculate the lag s difference or length s seasonal change series. For monthly or quarterly data, an appropriate value of s would be 12 or 4, respectively. The `diff()` function has `lag = 1` as its default for first differencing. Similar to before, a seasonally differenced series will have $s$ fewer observations than the original series.

```{r}
dy <- diff(y, lag = 4)

autoplot(dy) + 
  theme_minimal() +
  labs(x = "Time", y = "Index Value") 
```

Once again differencing allows you to remove the longer-term time trend - in this case, seasonal volatility - and focus on the change from one period to another.

# White Noise (W-N) Time Series

The white noise (WN) model is a basic time series model. It is also a basis for the more elaborate models we will consider. We will focus on the simplest form of WN, independent and identically distributed data.

The `arima.sim()` function can be used to simulate data from a variety of time series models. For now we note that the `ARIMA(0, 0, 0)` model is simply the WN model.

```{r}
# Simulate a WN model with list(order = c(0, 0, 0))
white_noise <- arima.sim(model = list(order = c(0, 0, 0)), n = 100)

autoplot(white_noise) + 
  theme_minimal() +
  geom_hline(yintercept = 0, color = "red", linetype = "dotted")  +
  labs(x = "Time", y = "Index Value") 
```

```{r}
# Simulate from the WN model with: mean = 100, sd = 10
white_noise_2 <- arima.sim(list(order = c(0, 0, 0)), n = 100, mean = 100, sd = 10)

autoplot(white_noise_2) + 
  theme_minimal() +
  geom_hline(yintercept = 100, color = "red", linetype = "dotted")  +
  labs(x = "Time", y = "Index Value") 
```

For a given time series $Y$ we can fit the white noise (WN) model using the `arima(..., order = c(0, 0, 0))` function. Recall that the WN model is an `ARIMA(0,0,0)` model. Applying the `arima()` function returns information or output about the estimated model. For the WN model this includes the estimated mean, labeled `intercept`, and the estimated variance, labeled `sigma^2`.

```{r}
y_ts <- ts(c(
  109.76134, 98.31610, 100.63295, 88.74340, 101.87238, 104.62836, 96.64462,
  102.86194, 112.76247, 82.23219, 88.94434, 94.60318, 105.61113, 113.82776,
  104.82319, 101.29629, 82.98459, 88.96058, 100.60046, 91.03525, 100.19286,
  95.32537, 94.58643, 121.35830, 87.37874, 96.89007, 90.51310, 99.90843,
  102.56934, 104.96144, 104.73464, 88.00465, 100.77943, 121.64776, 85.77083,
  79.15428, 98.18847, 99.90006, 98.91084, 101.64422, 102.79526, 84.65112,
  96.47870, 105.81547, 98.51869, 105.24366, 109.61264, 85.14201, 82.81442,
  103.03629, 93.56967, 98.08922, 81.25461, 109.18554, 80.43181, 103.55953,
  80.22269, 84.97477, 107.78363, 92.61288, 99.80293, 107.25085, 98.66378,
  91.92275, 98.32642, 112.73402, 96.02811, 92.64909, 83.08484, 97.22196,
  106.61361, 97.36943, 108.78465, 104.91858, 84.44343, 85.60786, 96.51529,
  94.18105, 85.02851, 63.26622, 87.22137, 103.52295, 105.60216, 103.26039,
  101.11519, 108.45697, 97.67631, 103.01081, 100.61756, 105.58108, 98.72722,
  98.43996, 90.78219, 92.74599, 102.93762, 83.95306, 110.15937, 104.17578,
  99.27876, 103.25115
))
```

```{r}
# Fit the WN model to y_ts using the arima command
arima(y_ts, order = c(0, 0, 0))

# Calculate the sample mean and sample variance of y_ts

mean(y_ts)
var(y_ts)
```

From the comparisons you can see that the arima() function estimates are very close to the sample mean and variance estimates, in fact identical for the mean.

# Random Walk (R-W) Time Series

The random walk (RW) model is also a basic time series model. It is the cumulative sum (or integration) of a mean zero white noise (WN) series, such that the first difference series of a RW is a WN series. Note for reference that the RW model is an `ARIMA(0, 1, 0)` model, in which the middle entry of 1 indicates that the model's order of integration is 1.

The arima.sim() function can be used to simulate data from the RW by including the model = list(order = c(0, 1, 0)) argument. We also need to specify a series length n. Finally, you can specify a sd for the series (increments), where the default value is 1.

```{r}
set.seed(123)
# Generate a RW model using arima.sim
random_walk <- arima.sim(model = list(order = c(0, 1, 0)), n = 100)

autoplot(random_walk) + 
  theme_minimal() +
  labs(x = "Time", y = "Index Value") 

```

```{r}
# Calculate the first difference series
random_walk_diff <- diff(random_walk)

autoplot(random_walk_diff) + 
  theme_minimal() +
  labs(x = "Time", y = "Index Value") 

```

The first difference of your random_walk data is white noise data. This is because a random walk is simply recursive white noise data. By removing the long-term trend, you end up with simple white noise.


A random walk (RW) need not wander about zero, it can have an upward or downward trajectory, i.e., a drift or time trend. This is done by including an intercept in the RW model, which corresponds to the slope of the RW time trend.

To simulate data from the RW model with a drift you again use the `arima.sim()` function with the `model = list(order = c(0, 1, 0))` argument. This time, you should add the additional argument `mean = ...` to specify the drift variable, or the intercept.

```{r}
set.seed(123)
# Generate a RW model with a drift uing arima.sim
rw_drift <- arima.sim(model = list(order = c(0, 1, 0)), n = 100, mean = 1)

autoplot(rw_drift) + 
  theme_minimal() +
  labs(x = "Time", y = "Index Value") 

```

```{r}
# Now fit the WN model to the differenced data
model_wn <- arima(random_walk_diff, order = c(0, 0, 0))

# Store the value of the estimated time trend (intercept)
int_wn <- model_wn$coef

# Plot the original random_walk data
autoplot(random_walk) + 
  theme_minimal() +
  labs(x = "Time", y = "Index Value") +
  # Use geom_abline() to add time trend to the figure
  geom_abline(slope = int_wn, intercept = 0, 
              color = "red", linetype = "dotted")

```

# Correlation analysis

Time series data is often presented in a time series plot. For example, the index values from the eu_stocks dataset are shown in the adjoining figure. 

```{r}
autoplot(EuStockMarkets) + 
  theme_minimal() +
  labs(x = "Year", y = "Index Value", title = "Major European Stock Indices, 1991-1998") +
  scale_x_discrete(limits = seq(1991, 1998))
```


It is also useful to examine the bivariate relationship between pairs of time series. Here we will consider the contemporaneous relationship, that is matching observations that occur at the same time, between pairs of index values as well as their log returns. 

The `plot(a, b)` function will produce a scatterplot when two time series names `a` and `b` are given as input.

Because the DAX and FTSE returns have similar time coverage, you can easily make a scatterplot of these indices. 

```{r}
head(EuStockMarkets)

# Make a scatterplot of DAX and FTSE
plot(EuStockMarkets[, 1], EuStockMarkets[, 4])
```


To simultaneously make scatterplots for all pairs of several assets the `pairs()` function can be applied to produce a scatterplot matrix. When shared time trends are present in prices or index values it is common to instead compare their returns or log returns.

```{r}
# Make a scatterplot matrix of EuStockMarkets
pairs(EuStockMarkets)
```

Log returns, also called continuously compounded returns, are also commonly used in financial time series analysis. They are the log of gross returns, or equivalently, the changes (or first differences) in the logarithm of prices.

```{r}
# Convert eu_stocks to log returns
logreturns <- diff(log(EuStockMarkets))

# Plot logreturns
plot(logreturns)

# Make a scatterplot matrix of logreturns
pairs(logreturns)

```


Note that the normal distribution has elliptical contours of equal probability, and pairs of data drawn from the multivariate normal distribution form a roughly elliptically shaped point cloud. Do any of the pairs in the scatterplot matrices exhibit this pattern, before or after log transformation?

Sample covariances measure the strength of the linear relationship between matched pairs of variables. The `cov()` function can be used to calculate covariances for a pair of variables, or a covariance matrix when a matrix containing several variables is given as input. For the latter case, the matrix is symmetric with covariances between variables on the off-diagonal and variances of the variables along the diagonal. 

```{r}
# Use cov() with DAX_logreturns and FTSE_logreturns
cov(logreturns[, 1], logreturns[, 4])

cov(logreturns)
```


Covariances are very important throughout finance, but they are not scale free and they can be difficult to directly interpret. Correlation is the standardized version of covariance that ranges in value from -1 to 1, where values close to 1 in magnitude indicate a strong linear relationship between pairs of variables. The `cor()` function can be applied to both pairs of variables as well as a matrix containing several variables, and the output is interpreted analogously.

```{r}
# Use cor() with DAX_logreturns and FTSE_logreturns
cor(logreturns[, 1], logreturns[, 4])

cor(logreturns)
```

# Autocorrelation Function (ACF)

Autocorrelation is a very powerful tool for time series analysis. It helps us study how each time series observation is related to its recent past. Processes with greater autocorrelation are more predictable than those with none.

For a time series $Y$ of length $T$ we consider the $T-1$ pairs of observations one time unit apart. The first such pair is ($Y_2, Y_1$), and the next is ($Y_3, Y_2$). Each such pair is of the form ($Y_t, Y_{t-1}$) where $t$ is the observation index, which we vary from 2 to $T$ in this case. The lag-1 autocorrelation of $Y$ can be estimated as the sample correlation of these ($Y_t, Y_{t-1}$) pairs.

Applying `acf(..., lag.max = 1, plot = FALSE)` to a series $Y$ automatically calculates the lag-1 autocorrelation.

Estimating the autocorrelation function (ACF) at many lags allows us to assess how a time series x relates to its past. The numeric estimates are important for detailed calculations, but it is also useful to visualize the ACF as a function of the lag.

In fact, the acf() command produces a figure by default. It also makes a default choice for lag.max, the maximum number of lags to be displayed.

```{r}
acf(dy, lag.max = 1, plot = FALSE)
acf(random_walk, lag.max = 1, plot = FALSE)

# Generate ACF estimates for x up to lag-10
acf(random_walk, lag.max = 10, plot = FALSE)

acf(random_walk)
```

 The random walk time series shows strong persistence, meaning the current value is closely relatively to those that proceed it. 

# Autoregressive Model

The autoregressive (AR) model is arguably the most widely used time series model. It shares the very familiar interpretation of a simple linear regression, but here each observation is regressed on the previous observation. The AR model also includes the white noise (WN) and random walk (RW) models.

The `arima.sim()` function can be used to simulate data from an AR model by setting the model argument equal to `list(ar = phi)` , in which `phi` is a slope parameter from the interval `(-1, 1)`. We also need to specify a series length `n`.

```{r}
# Simulate an AR model with 0.5 slope
x <- arima.sim(model = list(ar = 0.5), n = 100)

# Simulate an AR model with 0.9 slope

y <- arima.sim(model = list(ar = 0.9), n = 100)

# Simulate an AR model with -0.75 slope

z <- arima.sim(model = list(ar = -0.75), n = 100)

# Plot your simulated data
plot.ts(cbind(x, y, z))

```

Your $x$ data shows a just a moderate amount of autocorrelation while your $y$ data shows a large amount of autocorrelation. Alternatively, your $z$ data tends to oscillate considerably from one observation to the next.

Now, estimate the autocorrelation function (ACF) for an autoregression

```{r}
# Calculate the ACF for x
acf(x)

# Calculate the ACF for y
acf(y)

# Calculate the ACF for z
acf(z)
```

The plots generated by the acf() command provide useful information about each lag of your time series. The first series x has positive autocorrelation for the first couple lags, but they quickly approach zero. The second series y has positive autocorrelation for many lags, but they also decay to zero. The last series z has an alternating pattern, as does its autocorrelation function (ACF), but its ACF still quickly decays to zero in magnitude.

The random walk (RW) model is a special case of the autoregressive (AR) model, in which the slope parameter is equal to 1. Recall from previous chapters that the RW model is not stationary and exhibits very strong persistence. Its sample autocovariance function (ACF) also decays to zero very slowly, meaning past values have a long lasting impact on current values.

The stationary AR model has a slope parameter between -1 and 1. The AR model exhibits higher persistence when its slope parameter is closer to 1, but the process reverts to its mean fairly quickly. Its sample ACF also decays to zero at a quick (geometric) rate, indicating that values far in the past have little impact on future values of the process.

```{r}
# Simulate and plot AR model with slope 0.9 
x <- arima.sim(model = list(ar = 0.9), n = 200)
ts.plot(x)
acf(x)

# Simulate and plot AR model with slope 0.98
y <- arima.sim(model = list(ar = 0.98), n = 200)
ts.plot(y)
acf(y)

# Simulate and plot RW model
z <- arima.sim(model = list(order = c(0, 1, 0)), n = 200)
ts.plot(z)
acf(z)

```

As you can see, the AR model represented by series $y$ exhibits greater persistence than series $x$, but the ACF continues to decay to 0. By contrast, the RW model represented by series $z$ shows considerable persistence and relatively little decay in the ACF.

For a given time series x we can fit the autoregressive (AR) model using the `arima()` command and setting order equal to `c(1, 0, 0)`. Note for reference that an AR model is an ARIMA(1, 0, 0) model.

```{r}

# Fit the AR model to AirPassengers
AR <- arima(AirPassengers, order = c(1, 0, 0))
print(AR)

# Run the following commands to plot the series and fitted values
ts.plot(AirPassengers)
AR_fitted <- AirPassengers - residuals(AR)
points(AR_fitted, type = "l", col = 2, lty = 2)

```

By fitting an AR model to the AirPassengers data, you've succesfully modeled the data in a reproducible fashion. This allows you to predict future observations based on your AR_fitted data.

The `predict()` function can be used to make forecasts from an estimated AR model.

```{r}
# Use predict() to make a 1-step forecast
predict_AR <- predict(AR)

# Obtain the 1-step forecast using $pred[1]
predict_AR$pred[1]

# Use predict to make 1-step through 10-step forecasts
predict(AR, n.ahead = 10)

# Run to plot the Nile series plus the forecast and 95% prediction intervals
ts.plot(AirPassengers)
AR_forecast <- predict(AR, n.ahead = 10)$pred
AR_forecast_se <- predict(AR, n.ahead = 10)$se
points(AR_forecast, type = "l", col = 2)
points(AR_forecast - 2*AR_forecast_se, type = "l", col = 2, lty = 2)
points(AR_forecast + 2*AR_forecast_se, type = "l", col = 2, lty = 2)

```


In the object generated by your `predict()` command, the `$pred` value is the forecast, and the `$se` value is the standard error for the forecast.


The relatively wide band of confidence (represented by the dotted lines) is a result of the low persistence.


# Moving Average Model

The simple moving average (MA) model is a parsimonious time series model used to account for very short-run autocorrelation. It does have a regression like form, but here each observation is regressed on the previous innovation, which is not actually observed. Like the autoregressive (AR) model, the MA model includes the white noise (WN) model as special case.

As with previous models, the MA model can be simulated using the `arima.sim()` command by setting the model argument to `list(ma = theta)`, where theta is a slope parameter from the interval (-1, 1). Once again, you also need to specify the series length using the `n` argument.

```{r}
# Generate MA model with slope 0.5
x <- arima.sim(model = list(ma = 0.5), n = 100)

# Generate MA model with slope 0.9
y <- arima.sim(model = list(ma = 0.9), n = 100)

# Generate MA model with slope -0.5
z <- arima.sim(model = list(ma = -0.5), n = 100)

# Plot all three models together
plot.ts(cbind(x, y, z))
```

Note that there is some very short-run persistence for the positive slope values (x and y), and the series has a tendency to alternate when the slope value is negative (z).

```{r}
# Calculate ACF for x
acf(x)

# Calculate ACF for y
acf(y)

# Calculate ACF for z
acf(z)

```

The series x has positive sample autocorrelation at the first lag, but it is approximately zero at other lags. The series y has a larger sample autocorrelation at its first lag, but it is also approximately zero for the others. The series z has an alternating pattern, and its sample autocorrelation is negative at the first lag. However, similar to the others, it is approximately zero for all higher lags.

The next step is to fit the simple moving average (MA) model to some data using the `arima()` command. For a given time series x we can fit the simple moving average (MA) model using `arima(..., order = c(0, 0, 1))`. Note for reference that an MA model is an ARIMA(0, 0, 1) model.


```{r}
# Fit the MA model to Nile
MA <- arima(Nile, order = c(0, 0, 1))
print(MA)

# Plot Nile and MA_fit 
ts.plot(Nile)
MA_fit <- Nile - resid(MA)
points(MA_fit, type = "l", col = 2, lty = 2)

```

By fitting an MA model to your Nile data, you're able to capture variation in the data for future prediction. Based on the plot you've generated, does the MA model appear to be a strong fit for the Nile data?

```{r}
# Make a 1-step forecast based on MA
predict_MA <- predict(MA)

# Obtain the 1-step forecast using $pred[1]
predict_MA$pred[1]

# Make a 1-step through 10-step forecast based on MA
predict(MA, n.ahead = 10)

# Plot the Nile series plus the forecast and 95% prediction intervals
ts.plot(Nile, xlim = c(1871, 1980))
MA_forecasts <- predict(MA, n.ahead = 10)$pred
MA_forecast_se <- predict(MA, n.ahead = 10)$se
points(MA_forecasts, type = "l", col = 2)
points(MA_forecasts - 2*MA_forecast_se, type = "l", col = 2, lty = 2)
points(MA_forecasts + 2*MA_forecast_se, type = "l", col = 2, lty = 2)
```

Note that the MA model can only produce a 1-step forecast. For additional forecasting periods, the predict() command simply extends the original 1-step forecast. This explains the unexpected horizontal lines after 1971.


As you've seen, autoregressive (AR) and simple moving average (MA) are two useful approaches to modeling time series. But how can you determine whether an AR or MA model is more appropriate in practice?

To determine model fit, you can measure the Akaike information criterion (AIC) and Bayesian information criterion (BIC) for each model. 

```{r}
# Fit the AR model to Nile
AR <- arima(Nile, order = c(1, 0, 0))
AR_fit <- Nile - resid(AR)
```


```{r}
# Find correlation between AR_fit and MA_fit
cor(AR_fit, MA_fit)

# Find AIC of AR
AIC(AR)

# Find AIC of MA
AIC(MA)

# Find BIC of AR
BIC(AR)

# Find BIC of MA
BIC(MA)

```

Although the predictions from both models are very similar (indeed, they have a correlation coeffiicent of 0.94), both the AIC and BIC indicate that the AR model is a slightly better fit for your Nile data.